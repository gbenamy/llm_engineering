{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbenamy/projects/llm_engineering/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from typing import List\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPEN_API_KEY')\n",
    "MODEL = MODEL_GPT\n",
    "openai = OpenAI()\n",
    "\n",
    "system_prompt = \" You are a tuter in LLM learning course. \\\n",
    "I'm going to ask you questions and you will answer as a tuter, in the most clear and elapborated way.\\n\"\n",
    "user_prompt = \"Hi, here is my question:\\n\"\n",
    "\n",
    "def ai_answer(question):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt + question}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's break down the code you provided step by step:\n",
       "\n",
       "### Context:\n",
       "\n",
       "In Python, the `yield` statement is used within a generator function to produce a sequence of values lazily, meaning that values are generated only when requested, rather than all at once. The `yield from` expression allows a generator to yield all values from another iterable (like a list, tuple, dictionary, or another generator).\n",
       "\n",
       "### Breakdown of the Code:\n",
       "\n",
       "python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "1. **Set Comprehension**:\n",
       "   - The expression `{book.get(\"author\") for book in books if book.get(\"author\")}` is a **set comprehension**.\n",
       "   - It iterates over each `book` in the `books` collection.\n",
       "\n",
       "2. **Book Retrieval with `.get()` Method**:\n",
       "   - For each `book`, it attempts to retrieve the value associated with the key `\"author\"` using the `get()` method.\n",
       "   - The `get()` method is used because it safely retrieves the value; if the key does not exist, it returns `None` instead of raising a KeyError (which would happen if you used standard dictionary access, i.e., `book[\"author\"]`).\n",
       "\n",
       "3. **Condition with `if` Statement**:\n",
       "   - The `if book.get(\"author\")` condition ensures that only non-`None` authors are included in the set comprehension. This means that if there's no author for a particular book (i.e., `book.get(\"author\")` returns `None` or an empty string), that book will be ignored in the comprehension.\n",
       "\n",
       "4. **Set Creation**:\n",
       "   - The result of the comprehension `{...}` is a set that contains unique author names extracted from the `books`, with no duplicates due to the properties of a set.\n",
       "\n",
       "5. **Yielding Values**:\n",
       "   - The `yield from` statement then takes this set of author names and yields each one sequentially. This allows the generator to effectively return each unique author as a separate value when iterated over.\n",
       "\n",
       "### Summary:\n",
       "\n",
       "The overall purpose of this line of code is to produce a generator that yields unique author names from the collection of books. Each author name is generated lazily, meaning the next author is only fetched when the iterator is advanced (e.g., when calling `next()` on it or in a loop).\n",
       "\n",
       "This is a useful construct when you want to process or consume data in a memory-efficient manner, particularly when dealing with potentially large data sets, as it avoids creating a separate list or storing all authors in memory at once."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "ai_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
